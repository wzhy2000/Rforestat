df$AGB <- df$STEM + df$BRANCH + df$FOLIAGE + df$FRUIT
force(picea)
library(systemfit)
library(forestat)
df <- data(picea)
df$AGB <- df$STEM + df$BRANCH + df$FOLIAGE + df$FRUIT
force(picea)
df$AGB <- df$STEM + df$BRANCH + df$FOLIAGE + df$FRUIT
df <- data(picea)
library(systemfit)
library(forestat)
data(picea)
picea$AGB <- picea$STEM + picea$BRANCH + picea$FOLIAGE + picea$FRUIT
picea$DBH <- picea$D0
NDBH <- DBH ~ beta1 * exp(-beta2 * LH - beta3 * CPA)
NAGB <- AGB ~ alpha1 * DBH^alpha2 * LH^alpha3
models <- list(NDBH, NAGB)
instrument <- ~LH+CPA
startvalues<-c(beta1=3.119,beta2=-0.219,beta3=-0.044,alpha1=0.930,alpha2=1.737,alpha3=-0.125)
modele3.2sls<-nlsystemfit(method="2SLS",models,startvalues,inst=instrument,
data=picea)
modele3.3sls<-nlsystemfit(method="3SLS",models,startvalues,inst=instrument,
data=picea)
cat("===== 2SLS 参数估计 =====\n")
print(cbind(Estimate = modele3.2sls$b, StdError = modele3.2sls$se))
cat("\n===== 3SLS 参数估计 =====\n")
print(cbind(Estimate = modele3.3sls$b, StdError = modele3.3sls$se))
# ===== 2SLS 模型结果 =====
cat("【2SLS 模型】\n")
cat("方程 1 - R²:", round(modele3.2sls$eq[[1]]$r2, 5),
", RMSE:", round(modele3.2sls$eq[[1]]$rmse, 5), "\n")
cat("方程 2 - R²:", round(modele3.2sls$eq[[2]]$r2, 5),
", RMSE:", round(modele3.2sls$eq[[2]]$rmse, 5), "\n")
# ===== 3SLS 模型结果 =====
cat("\n【3SLS 模型】\n")
cat("方程 1 - R²:", round(modele3.3sls$eq[[1]]$r2, 5),
", RMSE:", round(modele3.3sls$eq[[1]]$rmse, 5), "\n")
cat("方程 2 - R²:", round(modele3.3sls$eq[[2]]$r2, 5),
", RMSE:", round(modele3.3sls$eq[[2]]$rmse, 5), "\n")
par(mfrow = c(2, 2))
plot(modele3.2sls$eq[[1]]$predicted, modele3.2sls$eq[[1]]$residuals,
xlab = "Fitted DBH", ylab = "Residuals", main = "2SLS - DBH", pch = 16)
abline(h = 0, col = "red")
plot(modele3.2sls$eq[[2]]$predicted, modele3.2sls$eq[[2]]$residuals,
xlab = "Fitted AGB", ylab = "Residuals", main = "2SLS - AGB", pch = 16)
abline(h = 0, col = "red")
plot(modele3.3sls$eq[[1]]$predicted, modele3.3sls$eq[[1]]$residuals,
xlab = "Fitted DBH", ylab = "Residuals", main = "3SLS - DBH", pch = 16)
abline(h = 0, col = "blue")
plot(modele3.3sls$eq[[2]]$predicted, modele3.3sls$eq[[2]]$residuals,
xlab = "Fitted AGB", ylab = "Residuals", main = "3SLS - AGB", pch = 16)
abline(h = 0, col = "blue")
library(systemfit)
library(plantecophys)
data(manyacidat)
df <- na.omit(manyacidat)  # 删除缺失值，避免计算错误
# 构建两个方程
eq1 <- Photo ~ Ci + Tleaf + PARi
eq2 <- Ci ~ Photo + Tleaf + PARi
eqs <- list(Photo = eq1, Ci = eq2)
# SUR 拟合
fit_surf <- systemfit(eqs, method = "SUR", data = df)
summary(fit_surf)
model_eval <- function(model) {
for (i in 1:length(model$eq)) {
name <- names(model$eq)[i]
y <- model$eq[[i]]$model[[1]]
yhat <- fitted(model$eq[[i]])
res <- residuals(model$eq[[i]])
SSE <- sum(res^2)
SST <- sum((y - mean(y))^2)
R2 <- 1 - SSE / SST
RMSE <- sqrt(mean(res^2))
MSE <- mean(res^2)
cat(paste0("\n[", name, "]\n"))
cat(sprintf("  R²    = %.4f\n", R2))
cat(sprintf("  MSE   = %.4f\n", MSE))
cat(sprintf("  RMSE  = %.4f\n", RMSE))
}
}
model_eval(fit_surf)
par(mfrow = c(2, 2))  # 设置2行2列图像
for (i in 1:2) {
name <- names(fit_surf$eq)[i]
y <- fit_surf$eq[[i]]$model[[1]]
yhat <- fitted(fit_surf$eq[[i]])
res <- residuals(fit_surf$eq[[i]])
plot(y, yhat, main = paste("拟合图 -", name),
xlab = "Observed", ylab = "Fitted", pch = 16, col = "steelblue")
abline(0, 1, col = "red", lty = 2)
# 残差图
plot(yhat, res, main = paste("残差图 -", name),
xlab = "Fitted", ylab = "Residuals", pch = 16, col = "gray40")
abline(h = 0, col = "red", lty = 2)
}
par(mfrow = c(1, 1))  # 恢复默认图像布局
library(fortedata)
library(rstanarm)
install.packages("rstanarm")
setwd("D:/大连理工大学/R语言书稿/exercise/chap13")
library(BGLR)
library(ggplot2)
library(tidyr)
# 数据准备
data <- read.csv("data.csv", fileEncoding = "gbk")
y <- data$BA
X_base <- as.matrix(data[, c("D0", "VH", "CD")])
X_inter <- cbind(X_base, D0_VH = data$D0 * data$VH)  # 添加交互项
# 模拟缺失用于预测精度评估
set.seed(123)
na.idx <- sample(1:length(y), 100)
y_miss <- y
y_miss[na.idx] <- NA
# 模型1：非信息先验建模（贝叶斯岭回归 BRR）
model1 <- BGLR(
y = y_miss,
ETA = list(list(X = X_base, model = "BRR")),
nIter = 1000, burnIn = 200, verbose = FALSE
)
# 模型2：偏置信念建模（BayesA + 交互项）
model2 <- BGLR(
y = y_miss,
ETA = list(list(X = X_inter, model = "BayesA")),
nIter = 1000, burnIn = 200, verbose = FALSE
)
# 模型预测与评估
yhat1 <- model1$yHat[na.idx]
yhat2 <- model2$yHat[na.idx]
ytrue <- y[na.idx]
cat("【模型评估】\n")
cat("BRR模型 → R² =", round(cor(yhat1, ytrue)^2, 3),
", RMSE =", round(sqrt(mean((yhat1 - ytrue)^2)), 3), "\n")
cat("BayesA模型（含交互）→ R² =", round(cor(yhat2, ytrue)^2, 3),
", RMSE =", round(sqrt(mean((yhat2 - ytrue)^2)), 3), "\n")
# 后验分布绘图：提取系数样本
b_samples <- model2$ETA[[1]]$b
# 判断是一维 or 多维
if (is.null(dim(b_samples))) {
# 单变量情况
df_post <- data.frame(变量 = "D0", 样本值 = b_samples)
ggplot(df_post, aes(x = 样本值)) +
geom_density(fill = "steelblue", alpha = 0.6) +
theme_minimal() +
labs(title = "参数后验密度图（单变量）", x = "回归系数", y = "密度")
} else {
# 多变量情况
colnames(b_samples) <- c("D0", "VH", "CD", "D0_VH")  # 可选：给变量命名
df_long <- pivot_longer(as.data.frame(b_samples),
cols = everything(),
names_to = "变量",
values_to = "样本值")
ggplot(df_long, aes(x = 样本值, fill = 变量)) +
geom_density(alpha = 0.6) +
facet_wrap(~变量, scales = "free") +
theme_minimal() +
labs(title = "参数估计的后验密度图", x = "回归系数", y = "密度")
}
library(e1071)
data <- read.csv("data.csv", fileEncoding = "gbk")
set.seed(42)
idx <- sample(1:nrow(data), 0.7 * nrow(data))
train <- data[idx, ]
test <- data[-idx, ]
model1 <- naiveBayes(as.factor(树种名) ~ D0 + BA + VH + CD, data = train)
pred1 <- predict(model1, test)
cat("【原始模型混淆矩阵】\n")
print(table(pred1, test$树种名))
cat("准确率 =", mean(pred1 == test$树种名), "\n")
model2 <- naiveBayes(as.factor(树种名) ~ D0 + BA + VH + CD, data = train, laplace = 1)
pred2 <- predict(model2, test)
cat("【加权模型混淆矩阵】\n")
print(table(pred2, test$树种名))
cat("准确率 =", mean(pred2 == test$树种名), "\n")
options(repos = c(CRAN = "https://mirrors.tuna.tsinghua.edu.cn/CRAN/"))
install.packages("MLmetrics")
library(MLmetrics)
f1_base <- F1_Score(y_pred = pred1, y_true = test$树种名, positive = "A")
f1_lap <- F1_Score(y_pred = pred2, y_true = test$树种名, positive = "A")
cat("原始模型 F1 =", f1_base, "，加权模型 F1 =", f1_lap, "\n")
wrong <- test[pred1 != test$树种名, ]
head(wrong[, c("树种名", "D0", "BA", "VH", "CD")], 5)
# install.packages("MLmetrics")
library(MLmetrics)
library(e1071)
data <- read.csv("data.csv", fileEncoding = "gbk")
set.seed(42)
idx <- sample(1:nrow(data), 0.7 * nrow(data))
train <- data[idx, ]
test <- data[-idx, ]
model1 <- naiveBayes(as.factor(树种名) ~ D0 + BA + VH + CD, data = train)
pred1 <- predict(model1, test)
cat("【原始模型混淆矩阵】\n")
print(table(pred1, test$树种名))
cat("准确率 =", mean(pred1 == test$树种名), "\n")
model2 <- naiveBayes(as.factor(树种名) ~ D0 + BA + VH + CD, data = train, laplace = 1)
pred2 <- predict(model2, test)
cat("【加权模型混淆矩阵】\n")
print(table(pred2, test$树种名))
cat("准确率 =", mean(pred2 == test$树种名), "\n")
options(repos = c(CRAN = "https://mirrors.tuna.tsinghua.edu.cn/CRAN/"))
f1_base <- F1_Score(y_pred = pred1, y_true = test$树种名, positive = "A")
f1_lap <- F1_Score(y_pred = pred2, y_true = test$树种名, positive = "A")
cat("原始模型 F1 =", f1_base, "，加权模型 F1 =", f1_lap, "\n")
wrong <- test[pred1 != test$树种名, ]
head(wrong[, c("树种名", "D0", "BA", "VH", "CD")], 5)
# install.packages("MLmetrics")
library(MLmetrics)
library(e1071)
data <- read.csv("data.csv", fileEncoding = "gbk")
set.seed(42)
idx <- sample(1:nrow(data), 0.7 * nrow(data))
train <- data[idx, ]
test <- data[-idx, ]
model1 <- naiveBayes(as.factor(树种名) ~ D0 + BA + VH + CD, data = train)
pred1 <- predict(model1, test)
cat("【原始模型混淆矩阵】\n")
print(table(pred1, test$树种名))
cat("准确率 =", mean(pred1 == test$树种名), "\n")
model2 <- naiveBayes(as.factor(树种名) ~ D0 + BA + VH + CD, data = train, laplace = 1)
pred2 <- predict(model2, test)
cat("【加权模型混淆矩阵】\n")
print(table(pred2, test$树种名))
cat("准确率 =", mean(pred2 == test$树种名), "\n")
options(repos = c(CRAN = "https://mirrors.tuna.tsinghua.edu.cn/CRAN/"))
f1_base <- F1_Score(y_pred = pred1, y_true = test$树种名, positive = "A")
f1_lap <- F1_Score(y_pred = pred2, y_true = test$树种名, positive = "A")
cat("原始模型 F1 =", f1_base, "，加权模型 F1 =", f1_lap, "\n")
wrong <- test[pred1 != test$树种名, ]
head(wrong[, c("树种名", "D0", "BA", "VH", "CD")], 5)
data("mite")
View(mite)
View(mite.env)
data(mite.env)
View(mite.env)
library(fortedata)
library(randomForest)
library(caret)
data <- fd_soil_respiration()
df <- na.omit(data.frame(
log_efflux = log(data$efflux),
soil_temp = data$soil_temp,
vwc = data$vwc
))
View(data)
colnames(data)
library(fortedata)
library(randomForest)
library(caret)
data <- fd_soil_respiration()
df <- na.omit(data.frame(
log_efflux = log(data$soil_co2_efflux),
soil_temp = data$soil_temp,
vwc = data$vwc
))
set.seed(123)
ctrl <- trainControl(method = "cv", number = 10)
model_rf <- train(
log_efflux ~ ., data = df,
method = "rf",
trControl = ctrl,
importance = TRUE
)
print(model_rf)
varImpPlot(model_rf$finalModel)
library(e1071)
library(caret)
data(Loblolly)
df <- Loblolly
df$Seed <- as.factor(df$Seed)
set.seed(123)
ctrl <- trainControl(method = "cv", number = 10)
grid <- expand.grid(sigma = c(0.01, 0.05), C = c(1, 10, 100))
model_svm <- train(
height ~ age + Seed, data = df,
method = "svmRadial",
trControl = ctrl,
tuneGrid = grid
)
print(model_svm)
plot(model_svm)
library(vegan)
library(caret)
data(mite)
data(mite.env)
mite.env$Shannon <- diversity(mite, index = "shannon")
df <- mite.env[, c("Shannon", "SubsDens", "WatrCont")]
set.seed(123)
ctrl <- trainControl(method = "cv", number = 10)
model_knn <- train(
Shannon ~ ., data = df,
method = "knn",
tuneLength = 10,
trControl = ctrl
)
print(model_knn)
plot(model_knn)
library(gbm)
library(randomForest)
library(caret)
library(forestat)
data(picea)
# 假设 pina 是已加载的数据
set.seed(123)
ctrl <- trainControl(method = "cv", number = 10)
# GBM
model_gbm <- train(
AGB ~ ., data = picea,
method = "gbm",
trControl = ctrl,
verbose = FALSE
)
library(gbm)
library(randomForest)
library(caret)
library(forestat)
data(picea)
# 假设 pina 是已加载的数据
set.seed(123)
ctrl <- trainControl(method = "cv", number = 10)
picea$AGB <- picea$STEM + picea$BRANCH + picea$FOLIAGE + picea$FRUIT
# GBM
model_gbm <- train(
AGB ~ ., data = picea,
method = "gbm",
trControl = ctrl,
verbose = FALSE
)
# RF
model_rf <- train(
AGB ~ ., data = picea,
method = "rf",
trControl = ctrl
)
# 对比
resamples(list(GBM = model_gbm, RF = model_rf)) %>% summary()
# 残差图
preds <- data.frame(
Actual = pina$AGB,
GBM = predict(model_gbm, pina),
RF = predict(model_rf, pina)
)
library(gbm)
library(randomForest)
library(caret)
library(forestat)
data(picea)
# 假设 pina 是已加载的数据
set.seed(123)
ctrl <- trainControl(method = "cv", number = 10)
picea$AGB <- picea$STEM + picea$BRANCH + picea$FOLIAGE + picea$FRUIT
# GBM
model_gbm <- train(
AGB ~ ., data = picea,
method = "gbm",
trControl = ctrl,
verbose = FALSE
)
# RF
model_rf <- train(
AGB ~ ., data = picea,
method = "rf",
trControl = ctrl
)
# 对比
resamples(list(GBM = model_gbm, RF = model_rf)) %>% summary()
# 残差图
preds <- data.frame(
Actual = picea$AGB,
GBM = predict(model_gbm, picea),
RF = predict(model_rf, picea)
)
plot(preds$Actual, preds$GBM - preds$Actual, main = "GBM 残差", ylab = "残差", xlab = "真实值")
# 加载必要包
library(caret)
library(e1071)
library(tidyverse)
library(forestat)
data("picea")
picea <- na.omit(picea)         # 删除缺失值
str(picea)
# 控制参数（10折交叉验证）
set.seed(123)
ctrl <- trainControl(method = "cv", number = 10)
# 核函数设定
kernels <- c("svmLinear", "svmPoly", "svmRadial")
kernel_names <- c("Linear", "Polynomial", "RBF")
# 批量训练 SVR 模型
models <- list()
for (i in seq_along(kernels)) {
model <- train(
AGB ~ LH+LCW, data = picea,
method = kernels[i],
trControl = ctrl,
preProcess = c("center", "scale"),
tuneLength = 5
)
models[[kernel_names[i]]] <- model
}
# 加载必要包
library(caret)
library(e1071)
library(tidyverse)
library(forestat)
data("picea")
picea <- na.omit(picea)         # 删除缺失值
str(picea)
picea$AGB <- picea$STEM + picea$BRANCH + picea$FOLIAGE + picea$FRUIT
# 控制参数（10折交叉验证）
set.seed(123)
ctrl <- trainControl(method = "cv", number = 10)
# 核函数设定
kernels <- c("svmLinear", "svmPoly", "svmRadial")
kernel_names <- c("Linear", "Polynomial", "RBF")
# 批量训练 SVR 模型
models <- list()
for (i in seq_along(kernels)) {
model <- train(
AGB ~ LH+LCW, data = picea,
method = kernels[i],
trControl = ctrl,
preProcess = c("center", "scale"),
tuneLength = 5
)
models[[kernel_names[i]]] <- model
}
# 汇总交叉验证结果（R²、RMSE 等）
results <- resamples(models)
summary(results)
# 可视化比较
dotplot(results, metric = "RMSE", main = "不同核函数SVR的RMSE比较")
dotplot(results, metric = "Rsquared", main = "不同核函数SVR的R²比较")
# 导出关键性能指标表格
performance_table <- results$values %>%
group_by(Resample, Model = .model) %>%
summarise(
RMSE = mean(RMSE),
Rsq = mean(Rsquared),
.groups = "drop"
)
View(results)
# 导出关键性能指标表格
performance_table <- results$values %>%
group_by(Resample, Model = models) %>%
summarise(
RMSE = mean(RMSE),
Rsq = mean(Rsquared),
.groups = "drop"
)
colnames(results$values)
# 导出关键性能指标表格
performance_table <- results$values %>%
group_by(Resample, Mode) %>%
summarise(
RMSE = mean(RMSE),
Rsq = mean(Rsquared),
.groups = "drop"
)
# 导出关键性能指标表格
performance_table <- results$values %>%
group_by(Resample, Model) %>%
summarise(
RMSE = mean(RMSE),
Rsq = mean(Rsquared),
.groups = "drop"
)
# 导出关键性能指标表格
performance_table <- results$values %>%
group_by(Resample) %>%
summarise(
RMSE = mean(RMSE),
Rsq = mean(Rsquared),
.groups = "drop"
)
View(results)
long_results <- results$values %>%
pivot_longer(cols = starts_with("Linear") | starts_with("Polynomial") | starts_with("RBF"),
names_to = "Model_Metric",
values_to = "Value") %>%
separate(Model_Metric, into = c("Model", "Metric"), sep = "~")
# 计算平均 RMSE 和 R²
performance_table <- long_results %>%
filter(Metric %in% c("RMSE", "Rsquared")) %>%
group_by(Resample, Model, Metric) %>%
summarise(
Mean_Value = mean(Value),
.groups = "drop"
)
print(performance_table)
# 导出关键性能指标表格
performance_table <- results$values %>%
group_by(Resample, Model = .model) %>%
summarise(
RMSE = mean(RMSE),
Rsq = mean(Rsquared),
.groups = "drop"
)
long_results <- results$values %>%
pivot_longer(cols = starts_with("Linear") | starts_with("Polynomial") | starts_with("RBF"),
names_to = "Model_Metric",
values_to = "Value") %>%
separate(Model_Metric, into = c("Model", "Metric"), sep = "~")
# 计算每个模型的RMSE和Rsquared的平均值
performance_table <- long_results %>%
filter(Metric %in% c("RMSE", "Rsquared")) %>%
group_by(Resample, Model, Metric) %>%
summarise(
Mean_Value = mean(Value),
.groups = "drop"
)
# 输出结果
performance_table
gc()
