getwd()
setwd("D:/大连理工大学/R语言书稿/新实验/整理")
pdf("机器学习ROC曲线.pdf", width = 8, height = 8)
plot(knn.roc.train, col="blue", legacy.axes = TRUE,  # 使用传统的 1 - specificity
mar = c(6, 6, 2, 2), mgp = c(4, 1, 0),cex.axis = 2.5, cex.lab = 2.5)
lines(rf.roc.train, col = "red")
lines(gbm.roc.train, col = "darkgreen")
lines(knn.roc.test, col = "#F5AE6B", lty = 17)
lines(rf.roc.test, col = "black", lty = 17)
lines(gbm.roc.test, col = "green", lty = 17)
legend("bottomright",
legend = c(
paste0("KNN train AUC = ", round(auc(knn.roc.train), 3)),
paste0("RF train AUC = ", round(auc(rf.roc.train), 3)),
paste0("GBM train AUC = ", round(auc(gbm.roc.train), 3)),
paste0("KNN test AUC = ", round(auc(knn.roc.test), 3)),
paste0("RF test AUC = ", round(auc(rf.roc.test), 3)),
paste0("GBM test AUC = ", round(auc(gbm.roc.test), 3))
),
lty = c(1,1,1,2,2,2),
col=c("blue", "red", "darkgreen", "#F5AE6B", "black","green" ), lwd = 2, cex = 1.5)
dev.off()
pdf("机器学习PR曲线.pdf", width = 8, height = 8)
par(mar = c(6, 6, 2, 2), mgp = c(4, 1, 0))
plot(knn.pr.train$curve[,1], knn.pr.train$curve[,2], type = "l", lwd = 2,
xlab = "Recall", ylab = "Precision", col = "blue", cex.axis = 2.5, cex.lab = 2.5)
lines(rf.pr.train$curve[,1], rf.pr.train$curve[,2], col = "red", lwd = 2)
lines(gbm.pr.train$curve[,1], gbm.pr.train$curve[,2], col = "darkgreen", lwd = 2)
lines(knn.pr.test$curve[,1], knn.pr.test$curve[,2], col = "#F5AE6B", , lty = 17, lwd = 2)
lines(rf.pr.test$curve[,1], rf.pr.test$curve[,2], col = "black", lty = 17, lwd = 2)
lines(gbm.pr.test$curve[,1], gbm.pr.test$curve[,2], col = "green", lty = 17, lwd = 2)
lines(x = c(0, 1), y = c(1, 0), col = "gray", lty = 3, lwd = 2)
legend("bottomleft",
legend = c(
paste0("KNN train AUC = ", round(knn.pr.train$auc.integral, 3)),
paste0("RF train AUC = ", round(rf.pr.train$auc.integral, 3)),
paste0("GBM train AUC = ", round(gbm.pr.train$auc.integral, 3)),
paste0("KNN test AUC = ", round(knn.pr.test$auc.integral, 3)),
paste0("RF test AUC = ", round(rf.pr.test$auc.integral, 3)),
paste0("GBM test AUC = ", round(gbm.pr.test$auc.integral, 3))
),
col = c("blue", "red", "darkgreen", "#F5AE6B", "black", "green"),
lty = c(1,1,1,2,2,2),
lwd = 2,
cex = 1.5)
dev.off()
resamps <- resamples(list(KNN = knn.mod, RF = rf.mod, GBM = gbm.mod))
pdf("AUC分布.pdf", width = 8, height = 8)
bwplot(resamps, metric="ROC", par.settings = list(
axis.text = list(cex = 2.5),     # 坐标轴刻度文字大小
par.main.text = list(cex = 2.5), # 主标题大小（如果有）
par.xlab.text = list(cex = 2.5), # x轴标签大小
par.ylab.text = list(cex = 2.5)  # y轴标签大小
))
dev.off()
# 模型选择
best.name <- results.test %>% arrange(desc(ROC_AUC)) %>% slice(1) %>% pull(Model)
cat("最优模型：", best.name, "\n")
best.mod <- switch(best.name,
"KNN" = knn.mod,
"RF" = rf.mod,
"GBM" = gbm.mod)
best.pred <- predict(best.mod, test)
cm <- confusionMatrix(best.pred, test$LiveDead, positive = "Dead")
print(cm)
# 结果可视化
best.prob <- predict(best.mod, test, type = "prob")[, "Dead"]
resid <- as.numeric(test$LiveDead == "Dead") - best.prob
df.res <- tibble(pred = best.prob, resid = resid)
# pdf("JFSP随机森林残差图.pdf", width = 8, height = 6, family = "GB1")
ggplot(df.res, aes(x = pred, y = resid)) +
geom_point(alpha = 0.6) +
geom_hline(yintercept = 0, linetype = "dashed") +
labs(x = "预测死亡概率",
y = "残差"
) +
theme_minimal() +
theme(
axis.title.x = element_text(size = 26, color = "black"),  # x轴标题字体大小
axis.title.y = element_text(size = 26, color = "black"),  # y轴标题字体大小
axis.text.x = element_text(size = 26, color = "black"),   # x轴文本字体大小
axis.text.y = element_text(size = 26, color = "black"),   # y轴文本字体大小
)
pdf("JFSP随机森林残差图.pdf", width = 8, height = 6, family = "GB1")
ggplot(df.res, aes(x = pred, y = resid)) +
geom_point(alpha = 0.6) +
geom_hline(yintercept = 0, linetype = "dashed") +
labs(x = "预测死亡概率",
y = "残差"
) +
theme_minimal() +
theme(
axis.title.x = element_text(size = 26, color = "black"),  # x轴标题字体大小
axis.title.y = element_text(size = 26, color = "black"),  # y轴标题字体大小
axis.text.x = element_text(size = 26, color = "black"),   # x轴文本字体大小
axis.text.y = element_text(size = 26, color = "black"),   # y轴文本字体大小
)
dev.off()
best.mod <- switch(best.name,
"KNN" = knn.mod,
"RF" = rf.mod,
"GBM" = gbm.mod)
best.pred <- predict(best.mod, test)
cm <- confusionMatrix(best.pred, test$LiveDead, positive = "Dead")
print(cm)
setwd("D:/大连理工大学/R语言书稿/Rforestat/R_code(new)/chap13")
library(caret)
data <- get(load("JFSP_all_df.rda"))
set.seed(123)
# 1. 先划分训练集和测试集（保留 LiveDead 标签的分布）
idx <- createDataPartition(data$LiveDead, p = 0.7, list = FALSE)
train.raw <- data[idx, ]
test.raw  <- data[-idx, ]
# 2. 对树种变量进行 one-hot 编码（只在训练集上构建 dummyVars 模型）
dmy <- dummyVars(~ Species, data = train.raw)
train.sp <- predict(dmy, train.raw)
test.sp  <- predict(dmy, test.raw)
# 3. 提取连续变量
num_vars <- c("heat_load", "slope", "roughness", "wyr1_suVPDmu")
train.num <- train.raw[, num_vars]
test.num  <- test.raw[, num_vars]
# 4. 在训练集上拟合 Z-score 标准化参数，并应用到测试集
preProcValues <- preProcess(train.num, method = c("center", "scale"))
train.num.scaled <- predict(preProcValues, train.num)
test.num.scaled  <- predict(preProcValues, test.num)
# 5. 合并连续变量 + one-hot 编码 + 响应变量
train <- as.data.frame(cbind(train.sp, train.num.scaled))
train$LiveDead <- factor(train.raw$LiveDead, levels = c(0,1), labels = c("Alive", "Dead"))
test <- as.data.frame(cbind(test.sp, test.num.scaled))
test$LiveDead <- factor(test.raw$LiveDead, levels = c(0,1), labels = c("Alive", "Dead"))
############### 1.knn模型 ##########
library(caret)
# 设置训练控制参数，使用 10 折交叉验证
trControl <- trainControl(method = "cv", number = 10)
# 定义参数调优网格，设置 K 的取值范围
tuneGrid <- expand.grid(k = 1:10)
# 训练 KNN 模型
modeli.knn <- train(
LiveDead ~ ., data = train,
method = "knn",
trControl = trControl,
tuneGrid = tuneGrid
)
# 对测试集进行预测
y.pred <- predict(modeli.knn, newdata = test)
# 生成混淆矩阵，评估模型性能
mat.conf <- confusionMatrix(y.pred, test$LiveDead)
print(mat.conf)
######### 2. 构建随机森林模型 ############
library(randomForest)
modeli.rf <- randomForest(LiveDead ~ ., data = train, importance=TRUE, proximity=TRUE)
print(modeli.rf)
y.pred <- predict(modeli.rf, newdata = test)
######### 3.梯度提升树模型 #############
set.seed(123)
library(gbm)
modeli.gbm <- gbm(LiveDead ~ ., data = train, distribution = "gaussian")
best.iter <- gbm.perf(modeli.gbm, method = "OOB")
print(modeli.gbm)
print(best.iter)
########### 4.多变量自适应回归样条 #############
set.seed(123)
library(earth)
modeli.earth <- earth(LiveDead ~ ., data = train)
summary(modeli.earth)
predictions <- predict(modeli.earth, newdata = test, type = "class")
########### 5.支持向量模型 #############
set.seed(123)
library(e1071)
modeli.svm <- svm(LiveDead ~ ., data = train, kernel = "linear")
summary(modeli.svm)
########### 6.神经网络模型 ###############
library(neuralnet)
set.seed(123)
modeli.nn <- neuralnet(LiveDead ~ ., data = train, hidden = c(5, 3),
linear.output = FALSE, stepmax = 100000)
plot(modeli.nn)
# 生成混淆矩阵，评估模型性能
mat.conf <- confusionMatrix(y.pred, test$LiveDead, positive = "Dead")
print(mat.conf)
library(caret)
data <- get(load("JFSP_all_df.rda"))
set.seed(123)
# 1. 先划分训练集和测试集（保留 LiveDead 标签的分布）
idx <- createDataPartition(data$LiveDead, p = 0.7, list = FALSE)
train.raw <- data[idx, ]
test.raw  <- data[-idx, ]
# 2. 对树种变量进行 one-hot 编码（只在训练集上构建 dummyVars 模型）
dmy <- dummyVars(~ Species, data = train.raw)
train.sp <- predict(dmy, train.raw)
test.sp  <- predict(dmy, test.raw)
# 3. 提取连续变量
num_vars <- c("heat_load", "slope", "roughness", "wyr1_suVPDmu")
train.num <- train.raw[, num_vars]
test.num  <- test.raw[, num_vars]
# 4. 在训练集上拟合 Z-score 标准化参数，并应用到测试集
preProcValues <- preProcess(train.num, method = c("center", "scale"))
train.num.scaled <- predict(preProcValues, train.num)
test.num.scaled  <- predict(preProcValues, test.num)
# 5. 合并连续变量 + one-hot 编码 + 响应变量
train <- as.data.frame(cbind(train.sp, train.num.scaled))
train$LiveDead <- factor(train.raw$LiveDead, levels = c(0,1), labels = c("Alive", "Dead"))
test <- as.data.frame(cbind(test.sp, test.num.scaled))
test$LiveDead <- factor(test.raw$LiveDead, levels = c(0,1), labels = c("Alive", "Dead"))
############### 1.knn模型 ##########
library(caret)
# 设置训练控制参数，使用 10 折交叉验证
trControl <- trainControl(method = "cv", number = 10)
# 定义参数调优网格，设置 K 的取值范围
tuneGrid <- expand.grid(k = 1:10)
# 训练 KNN 模型
modeli.knn <- train(
LiveDead ~ ., data = train,
method = "knn",
trControl = trControl,
tuneGrid = tuneGrid
)
# 对测试集进行预测
y.pred <- predict(modeli.knn, newdata = test)
# 生成混淆矩阵，评估模型性能
mat.conf <- confusionMatrix(y.pred, test$LiveDead, positive = "Dead")
print(mat.conf)
?gbm
modeli.gbm <- gbm(LiveDead ~ ., data = train, distribution = "bernoulli")
library(caret)
data <- get(load("JFSP_all_df.rda"))
set.seed(123)
# 1. 先划分训练集和测试集（保留 LiveDead 标签的分布）
idx <- createDataPartition(data$LiveDead, p = 0.7, list = FALSE)
train.raw <- data[idx, ]
test.raw  <- data[-idx, ]
# 2. 对树种变量进行 one-hot 编码（只在训练集上构建 dummyVars 模型）
dmy <- dummyVars(~ Species, data = train.raw)
train.sp <- predict(dmy, train.raw)
test.sp  <- predict(dmy, test.raw)
# 3. 提取连续变量
num_vars <- c("heat_load", "slope", "roughness", "wyr1_suVPDmu")
train.num <- train.raw[, num_vars]
test.num  <- test.raw[, num_vars]
# 4. 在训练集上拟合 Z-score 标准化参数，并应用到测试集
preProcValues <- preProcess(train.num, method = c("center", "scale"))
train.num.scaled <- predict(preProcValues, train.num)
test.num.scaled  <- predict(preProcValues, test.num)
# 5. 合并连续变量 + one-hot 编码 + 响应变量
train <- as.data.frame(cbind(train.sp, train.num.scaled))
test <- as.data.frame(cbind(test.sp, test.num.scaled))
View(train)
View(train)
View(data)
modeli.gbm <- gbm(LiveDead ~ ., data = train, distribution = "gaussian")
library(caret)
data <- get(load("JFSP_all_df.rda"))
set.seed(123)
# 1. 先划分训练集和测试集（保留 LiveDead 标签的分布）
idx <- createDataPartition(data$LiveDead, p = 0.7, list = FALSE)
train.raw <- data[idx, ]
test.raw  <- data[-idx, ]
# 2. 对树种变量进行 one-hot 编码（只在训练集上构建 dummyVars 模型）
dmy <- dummyVars(~ Species, data = train.raw)
train.sp <- predict(dmy, train.raw)
test.sp  <- predict(dmy, test.raw)
# 3. 提取连续变量
num_vars <- c("heat_load", "slope", "roughness", "wyr1_suVPDmu")
train.num <- train.raw[, num_vars]
test.num  <- test.raw[, num_vars]
# 4. 在训练集上拟合 Z-score 标准化参数，并应用到测试集
preProcValues <- preProcess(train.num, method = c("center", "scale"))
train.num.scaled <- predict(preProcValues, train.num)
test.num.scaled  <- predict(preProcValues, test.num)
# 5. 合并连续变量 + one-hot 编码 + 响应变量
train <- as.data.frame(cbind(train.sp, train.num.scaled))
train$LiveDead <- factor(train.raw$LiveDead, levels = c(0,1), labels = c("Alive", "Dead"))
test <- as.data.frame(cbind(test.sp, test.num.scaled))
test$LiveDead <- factor(test.raw$LiveDead, levels = c(0,1), labels = c("Alive", "Dead"))
modeli.gbm <- gbm(LiveDead ~ ., data = train, distribution = "gaussian")
library(caret)
data <- get(load("JFSP_all_df.rda"))
set.seed(123)
# 1. 先划分训练集和测试集（保留 LiveDead 标签的分布）
idx <- createDataPartition(data$LiveDead, p = 0.7, list = FALSE)
train.raw <- data[idx, ]
test.raw  <- data[-idx, ]
# 2. 对树种变量进行 one-hot 编码（只在训练集上构建 dummyVars 模型）
dmy <- dummyVars(~ Species, data = train.raw)
train.sp <- predict(dmy, train.raw)
test.sp  <- predict(dmy, test.raw)
# 3. 提取连续变量
num_vars <- c("heat_load", "slope", "roughness", "wyr1_suVPDmu")
train.num <- train.raw[, num_vars]
test.num  <- test.raw[, num_vars]
# 4. 在训练集上拟合 Z-score 标准化参数，并应用到测试集
preProcValues <- preProcess(train.num, method = c("center", "scale"))
train.num.scaled <- predict(preProcValues, train.num)
test.num.scaled  <- predict(preProcValues, test.num)
# 5. 合并连续变量 + one-hot 编码 + 响应变量
train <- as.data.frame(cbind(train.sp, train.num.scaled))
train$LiveDead <- factor(train.raw$LiveDead, levels = c(0,1), labels = c("Alive", "Dead"))
test <- as.data.frame(cbind(test.sp, test.num.scaled))
test$LiveDead <- factor(test.raw$LiveDead, levels = c(0,1), labels = c("Alive", "Dead"))
############### 1.knn模型 ##########
library(caret)
# 设置训练控制参数，使用 10 折交叉验证
trControl <- trainControl(method = "cv", number = 10)
# 定义参数调优网格，设置 K 的取值范围
tuneGrid <- expand.grid(k = 1:10)
# 训练 KNN 模型
modeli.knn <- train(
LiveDead ~ ., data = train,
method = "knn",
trControl = trControl,
tuneGrid = tuneGrid
)
# 对测试集进行预测
y.pred <- predict(modeli.knn, newdata = test)
# 生成混淆矩阵，评估模型性能
mat.conf <- confusionMatrix(y.pred, test$LiveDead, positive = "Dead")
print(mat.conf)
######### 2. 构建随机森林模型 ############
library(randomForest)
modeli.rf <- randomForest(LiveDead ~ ., data = train, importance=TRUE, proximity=TRUE)
print(modeli.rf)
y.pred <- predict(modeli.rf, newdata = test)
y.pred
str(train)
modeli.gbm <- gbm(LiveDead ~ ., data = train, distribution = "bernoulli")
set.seed(123)
library(gbm)
train$LiveDead <- ifelse(train$LiveDead == "Dead", 1, 0)
test$LiveDead  <- ifelse(test$LiveDead == "Dead", 1, 0)
modeli.gbm <- gbm(LiveDead ~ ., data = train, distribution = "bernoulli")
best.iter <- gbm.perf(modeli.gbm, method = "OOB")
print(modeli.gbm)
print(best.iter)
########### 4.多变量自适应回归样条 #############
set.seed(123)
library(earth)
modeli.earth <- earth(LiveDead ~ ., data = train)
summary(modeli.earth)
predictions <- predict(modeli.earth, newdata = test, type = "class")
View(predictions)
set.seed(123)
library(earth)
train$LiveDead <- factor(train.raw$LiveDead, levels = c(0,1), labels = c("Alive", "Dead"))
test$LiveDead <- factor(test.raw$LiveDead, levels = c(0,1), labels = c("Alive", "Dead"))
modeli.earth <- earth(LiveDead ~ ., data = train)
summary(modeli.earth)
predictions <- predict(modeli.earth, newdata = test, type = "class")
summary(modeli.svm)
library(caret)
data <- get(load("JFSP_all_df.rda"))
set.seed(123)
# 1. 先划分训练集和测试集（保留 LiveDead 标签的分布）
idx <- createDataPartition(data$LiveDead, p = 0.7, list = FALSE)
train.raw <- data[idx, ]
test.raw  <- data[-idx, ]
# 2. 对树种变量进行 one-hot 编码（只在训练集上构建 dummyVars 模型）
dmy <- dummyVars(~ Species, data = train.raw)
train.sp <- predict(dmy, train.raw)
test.sp  <- predict(dmy, test.raw)
# 3. 提取连续变量
num_vars <- c("heat_load", "slope", "roughness", "wyr1_suVPDmu")
train.num <- train.raw[, num_vars]
test.num  <- test.raw[, num_vars]
# 4. 在训练集上拟合 Z-score 标准化参数，并应用到测试集
preProcValues <- preProcess(train.num, method = c("center", "scale"))
train.num.scaled <- predict(preProcValues, train.num)
test.num.scaled  <- predict(preProcValues, test.num)
# 5. 合并连续变量 + one-hot 编码 + 响应变量
train <- as.data.frame(cbind(train.sp, train.num.scaled))
train$LiveDead <- factor(train.raw$LiveDead, levels = c(0,1), labels = c("Alive", "Dead"))
test <- as.data.frame(cbind(test.sp, test.num.scaled))
test$LiveDead <- factor(test.raw$LiveDead, levels = c(0,1), labels = c("Alive", "Dead"))
############### 1.knn模型 ##########
library(caret)
# 设置训练控制参数，使用 10 折交叉验证
trControl <- trainControl(method = "cv", number = 10)
# 定义参数调优网格，设置 K 的取值范围
tuneGrid <- expand.grid(k = 1:10)
# 训练 KNN 模型
modeli.knn <- train(
LiveDead ~ ., data = train,
method = "knn",
trControl = trControl,
tuneGrid = tuneGrid
)
# 对测试集进行预测
y.pred <- predict(modeli.knn, newdata = test)
# 生成混淆矩阵，评估模型性能
mat.conf <- confusionMatrix(y.pred, test$LiveDead, positive = "Dead")
print(mat.conf)
######### 2. 构建随机森林模型 ############
library(randomForest)
modeli.rf <- randomForest(LiveDead ~ ., data = train, importance=TRUE, proximity=TRUE)
print(modeli.rf)
y.pred <- predict(modeli.rf, newdata = test)
######### 3.梯度提升树模型 #############
set.seed(123)
library(gbm)
train$LiveDead <- ifelse(train$LiveDead == "Dead", 1, 0)
test$LiveDead  <- ifelse(test$LiveDead == "Dead", 1, 0)
modeli.gbm <- gbm(LiveDead ~ ., data = train, distribution = "bernoulli")
best.iter <- gbm.perf(modeli.gbm, method = "OOB")
print(modeli.gbm)
print(best.iter)
########### 4.多变量自适应回归样条 #############
set.seed(123)
library(earth)
train$LiveDead <- factor(train.raw$LiveDead, levels = c(0,1), labels = c("Alive", "Dead"))
test$LiveDead <- factor(test.raw$LiveDead, levels = c(0,1), labels = c("Alive", "Dead"))
modeli.earth <- earth(LiveDead ~ ., data = train)
summary(modeli.earth)
predictions <- predict(modeli.earth, newdata = test, type = "class")
########### 5.支持向量模型 #############
set.seed(123)
library(e1071)
modeli.svm <- svm(LiveDead ~ ., data = train, kernel = "linear")
summary(modeli.svm)
library(neuralnet)
set.seed(123)
modeli.nn <- neuralnet(LiveDead ~ ., data = train, hidden = c(5, 3),
linear.output = FALSE, stepmax = 100000)
plot(modeli.nn)
plot(modeli.nn)
?plot
?plot.neuralnet
?\plot.nn
?plot.nn
plot(modeli.nn, show.weights = FALSE)
pdf("基于神经网络的苗木存活状态分类模型结构.pdf", width = 8, height = 8)
pdf("基于神经网络的苗木存活状态分类模型结构.pdf", width = 8, height = 8)
plot(modeli.nn, show.weights = FALSE)
dev.off()
library(caret)
data <- get(load("JFSP_all_df.rda"))
set.seed(123)
# 1. 先划分训练集和测试集（保留 LiveDead 标签的分布）
idx <- createDataPartition(data$LiveDead, p = 0.7, list = FALSE)
train.raw <- data[idx, ]
test.raw  <- data[-idx, ]
# 2. 对树种变量进行 one-hot 编码（只在训练集上构建 dummyVars 模型）
dmy <- dummyVars(~ Species, data = train.raw)
train.sp <- predict(dmy, train.raw)
test.sp  <- predict(dmy, test.raw)
# 3. 提取连续变量
num_vars <- c("heat_load", "slope", "roughness", "wyr1_suVPDmu")
train.num <- train.raw[, num_vars]
test.num  <- test.raw[, num_vars]
# 4. 在训练集上拟合 Z-score 标准化参数，并应用到测试集
preProcValues <- preProcess(train.num, method = c("center", "scale"))
train.num.scaled <- predict(preProcValues, train.num)
test.num.scaled  <- predict(preProcValues, test.num)
# 5. 合并连续变量 + one-hot 编码 + 响应变量
train <- as.data.frame(cbind(train.sp, train.num.scaled))
train$LiveDead <- factor(train.raw$LiveDead, levels = c(0,1), labels = c("Alive", "Dead"))
test <- as.data.frame(cbind(test.sp, test.num.scaled))
test$LiveDead <- factor(test.raw$LiveDead, levels = c(0,1), labels = c("Alive", "Dead"))
############### 1.knn模型 ##########
library(caret)
# 设置训练控制参数，使用 10 折交叉验证
trControl <- trainControl(method = "cv", number = 10)
# 定义参数调优网格，设置 K 的取值范围
tuneGrid <- expand.grid(k = 1:10)
# 训练 KNN 模型
modeli.knn <- train(
LiveDead ~ ., data = train,
method = "knn",
trControl = trControl,
tuneGrid = tuneGrid
)
# 对测试集进行预测
y.pred <- predict(modeli.knn, newdata = test)
# 生成混淆矩阵，评估模型性能
mat.conf <- confusionMatrix(y.pred, test$LiveDead, positive = "Dead")
print(mat.conf)
######### 2. 构建随机森林模型 ############
library(randomForest)
modeli.rf <- randomForest(LiveDead ~ ., data = train, importance=TRUE, proximity=TRUE)
print(modeli.rf)
y.pred <- predict(modeli.rf, newdata = test)
######### 3.梯度提升树模型 #############
set.seed(123)
library(gbm)
train$LiveDead <- ifelse(train$LiveDead == "Dead", 1, 0)
test$LiveDead  <- ifelse(test$LiveDead == "Dead", 1, 0)
modeli.gbm <- gbm(LiveDead ~ ., data = train, distribution = "bernoulli")
best.iter <- gbm.perf(modeli.gbm, method = "OOB")
print(modeli.gbm)
print(best.iter)
########### 4.多变量自适应回归样条 #############
set.seed(123)
library(earth)
train$LiveDead <- factor(train.raw$LiveDead, levels = c(0,1), labels = c("Alive", "Dead"))
test$LiveDead <- factor(test.raw$LiveDead, levels = c(0,1), labels = c("Alive", "Dead"))
modeli.earth <- earth(LiveDead ~ ., data = train)
summary(modeli.earth)
predictions <- predict(modeli.earth, newdata = test, type = "class")
########### 5.支持向量模型 #############
set.seed(123)
library(e1071)
modeli.svm <- svm(LiveDead ~ ., data = train, kernel = "linear")
summary(modeli.svm)
########### 6.神经网络模型 ###############
library(neuralnet)
set.seed(123)
modeli.nn <- neuralnet(LiveDead ~ ., data = train, hidden = c(5, 3),
linear.output = FALSE, stepmax = 100000)
print(plot(modeli.nn, show.weights = FALSE))
plot(modeli.nn, show.weights = FALSE, col.entry = "blue", col.hidden = "green", col.out = "red", arrow.length = 0.2, cex = 0.8, information = FALSE, show.weights = FALSE)
plot(modeli.nn, col.entry = "blue", col.hidden = "green", col.out = "red", arrow.length = 0.2, cex = 0.8, information = FALSE, show.weights = FALSE)
plot(modeli.nn, col.entry = "blue", col.hidden = "green", col.out = "red", arrow.length = 0.2, cex = 0.8, information = FALSE, show.weights = FALSE)
plot(modeli.nn, col.entry = "blue", col.hidden = "green", col.out = "red", arrow.length = 0.2, cex = 0.8, information = FALSE, show.weights = FALSE)
plot(modeli.nn, col.entry = "blue", col.hidden = "green", col.out = "red", arrow.length = 0.2, cex = 0.8, information = FALSE, show.weights = FALSE)
