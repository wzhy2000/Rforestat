library(randomForest)
library(forestat)
data("crassifolia")
# AGB = Stem + Branch + Foliage + Fruit
crassifolia$AGB = crassifolia$Stem + crassifolia$Branch + crassifolia$Foliage + crassifolia$Fruit
set.seed(123)
datapartde <- sample(nrow(crassifolia), 0.7 * nrow(crassifolia))
train_data <- crassifolia[datapartde, ]
test_data <- crassifolia[-datapartde, ]
train_x <- train_data[, c("LH", "LHCB", "CPA", "D0", "H0", "HCB0", "CW")]
train_y <- train_data$AGB
test_x <- test_data[, c("LH", "LHCB", "CPA", "D0", "H0", "HCB0", "CW")]
test_y <- test_data$AGB
value <- seq(1, 7, by = 1)
rfModel_bestmtry <- list()
for (i in value) {
set.seed(123)
train_rf <- randomForest(x = train_x, y = train_y, ntree = 1000, mtry = i,
importance = TRUE, proximity = TRUE)
rfModel_bestmtry[[i]] <- train_rf$rsq[[1000]]
i <- i + 1
}
best_mtry <- which.max(rfModel_bestmtry) # best_mtry = 7
set.seed(123)
randomForest_model <- randomForest(x = train_x, y = train_y, ntree = 1000, mtry = 7, importance = TRUE, proximity = TRUE)
predictions <- predict(randomForest_model, newdata = test_data)
FittingEvaluationIndex(predictions, test_data$AGB)
rf.res <- predictions - test_data$AGB
data.rf <- data.frame(x = predictions, y = rf.res)
p.rf <- ggplot(data.rf, aes(x = x, y = y)) +
theme_light() +
geom_point(color = "steelblue", size = 3, show.legend = F) +
geom_hline(yintercept = c(0)) +
geom_vline(xintercept = c(0)) +
scale_x_continuous(limits = c(0, 600)) +
scale_y_continuous(limits = c(-50, 50)) +
labs(x = "Ground extimated AGB(g/m2)", y = "Residual") +
theme(text = element_text(size = 24))
pdf("exp13-3残差图.pdf", width = 8, height = 6)
p.rf
dev.off()
pdf("exp13-2残差图.pdf", width = 8, height = 6)
p.rf
dev.off()
pdf("exp13-3残差图.pdf", width = 8, height = 6)
p.gbm
dev.off()
?svm
?svm
set.seed(123)
svm_model <- svm(AGB ~ LH + LHCB + CPA + D0 + H0 + HCB0 + CW,
data = train_data, type = "eps-regression",
kernel = "radial", scale = TRUE)
library(forestat)     # 雷达反演数据集
library(caret)        # KNN
library(randomForest) # 随机森林
library(gbm)          # 梯度提升树
library(e1071)        # 支持向量机
data("crassifolia")   # 加载雷达反演数据集
# 数据处理
# AGB = Stem + Branch + Foliage + Fruit
crassifolia$AGB = crassifolia$Stem + crassifolia$Branch + crassifolia$Foliage + crassifolia$Fruit
set.seed(123)
datapartde <- sample(nrow(crassifolia), 0.7 * nrow(crassifolia))
train_data <- crassifolia[datapartde, ]
test_data <- crassifolia[-datapartde, ]
train_x <- train_data[, c("LH", "LHCB", "CPA", "D0", "H0", "HCB0", "CW")]
train_y <- train_data$AGB
test_x <- test_data[, c("LH", "LHCB", "CPA", "D0", "H0", "HCB0", "CW")]
test_y <- test_data$AGB
set.seed(123)
svm_model <- svm(AGB ~ LH + LHCB + CPA + D0 + H0 + HCB0 + CW,
data = train_data, type = "eps-regression",
kernel = "radial", scale = TRUE)
# 预测
predictions <- predict(svm_model, newdata = test_x)
mse <- mean((predictions - test_y)^2)
rsq <- cor(predictions, test_y)^2
cat("测试集 MSE：", mse, "\n")
cat("测试集 R²：", rsq, "\n")
set.seed(123)
randomforest_model <- randomForest(x = train_x, y = train_y, ntree = 500,
replace = TRUE, importance = TRUE)
predictions <- predict(randomforest_model, newdata = test_x)
mse <- mean((predictions - test_y)^2)
rsq <- cor(predictions, test_y)^2
cat("测试集 MSE：", mse, "\n")
cat("测试集 R²：", rsq, "\n")
set.seed(123)
# 使用5折交叉验证
gbm_model_cv <- gbm(formula = AGB ~ LH + LHCB + CPA + D0 + H0 + HCB0 + CW,
data = train_data, distribution = "gaussian",
n.trees = 1000, interaction.depth = 4, shrinkage = 0.01,
cv.folds = 5, verbose = FALSE)
best_trees <- gbm.perf(gbm_model_cv, method = "cv", plot.it = TRUE)
cat("最佳子树数量为：", best_trees, "\n")
predictions <- predict(gbm_model_cv, newdata = test_x)
mse <- mean((predictions - test_y)^2)
rsq <- cor(predictions, test_y)^2
cat("测试集 MSE：", mse, "\n")
cat("测试集 R²：", rsq, "\n")
set.seed(123)
svm_model <- svm(AGB ~ LH + LHCB + CPA + D0 + H0 + HCB0 + CW,
data = train_data, type = "eps-regression",
kernel = "radial", scale = TRUE)
# 预测
predictions <- predict(svm_model, newdata = test_x)
mse <- mean((predictions - test_y)^2)
rsq <- cor(predictions, test_y)^2
cat("测试集 MSE：", mse, "\n")
cat("测试集 R²：", rsq, "\n")
################# 4.支持向量模型 #####################
set.seed(123)
svm_model <- svm(AGB ~ LH + LHCB + CPA + D0 + H0 + HCB0 + CW,
data = train_data, type = "eps-regression",
kernel = "radial", scale = TRUE)
# 预测
predictions <- predict(svm_model, newdata = test_x)
mse <- mean((predictions - test_y)^2)
rsq <- cor(predictions, test_y)^2
cat("测试集 MSE：", mse, "\n")
cat("测试集 R²：", rsq, "\n")
svm_tune <- tune.svm(AGB ~ LH + LHCB + CPA + D0 + H0 + HCB0 + CW,
data = train_data, type = "eps-regression",
kernel = "radial", cost = 10^(-1:2), gamma = 10^(-2:1),
epsilon = c(0.1, 0.2, 0.3))
best_svm_model <- svm_tune$best.model
summary(best_svm_model)
predictions <- predict(best_svm_model, newdata = test_x)
mse <- mean((predictions - test_y)^2)
rsq <- cor(predictions, test_y)^2
cat("测试集 MSE：", mse, "\n")
cat("测试集 R²：", rsq, "\n")
########### 4.支持向量模型 #############
set.seed(123)
model <- svm(Species ~ ., data = iris, kernel = "linear")
summary(model)
library(forestat)
library(e1071)
data("crassifolia")
# 数据处理
# AGB = Stem + Branch + Foliage + Fruit
crassifolia$AGB = crassifolia$Stem + crassifolia$Branch + crassifolia$Foliage + crassifolia$Fruit
set.seed(123)
datapartde <- sample(nrow(crassifolia), 0.7 * nrow(crassifolia))
train_data <- crassifolia[datapartde, ]
test_data <- crassifolia[-datapartde, ]
train_x <- train_data[, c("LH", "LHCB", "CPA", "D0", "H0", "HCB0", "CW")]
train_y <- train_data$AGB
test_x <- test_data[, c("LH", "LHCB", "CPA", "D0", "H0", "HCB0", "CW")]
test_y <- test_data$AGB
?svm
gamma = 10^(-2:1)
svm_tune <- tune.svm(AGB ~ LH + LHCB + CPA + D0 + H0 + HCB0 + CW,
data = train_data, type = "eps-regression",
trControl =  trainControl(method = "repeatedcv", repeats = 2, number = 10),
kernel = "radial", cost = 10^(-1:2), gamma = 10^(-2:1),
epsilon = c(0.1, 0.2, 0.3))
best_svm_model <- svm_tune$best.model
summary(best_svm_model)
predictions <- predict(best_svm_model, data = test_data)
predictions
predictions <- predict(best_svm_model, data = test_x)
predictions <- predict(best_svm_model, data = test_data)
predictions <- predict(best_svm_model, data = test_x)
a <- as.data.frame(predictions )
View(a)
dim(a)
predictions <- predict(best_svm_model, newdata = test_data)
library(forestat)
library(e1071)
data("crassifolia")
# 数据处理
# AGB = Stem + Branch + Foliage + Fruit
crassifolia$AGB = crassifolia$Stem + crassifolia$Branch + crassifolia$Foliage + crassifolia$Fruit
set.seed(123)
datapartde <- sample(nrow(crassifolia), 0.7 * nrow(crassifolia))
train_data <- crassifolia[datapartde, ]
test_data <- crassifolia[-datapartde, ]
train_x <- train_data[, c("LH", "LHCB", "CPA", "D0", "H0", "HCB0", "CW")]
train_y <- train_data$AGB
test_x <- test_data[, c("LH", "LHCB", "CPA", "D0", "H0", "HCB0", "CW")]
test_y <- test_data$AGB
svm_tune <- tune.svm(AGB ~ LH + LHCB + CPA + D0 + H0 + HCB0 + CW,
data = train_data, type = "eps-regression",
kernel = "radial", cost = 10^(-1:2), gamma = 10^(-2:1),
epsilon = c(0.1, 0.2, 0.3))
best_svm_model <- svm_tune$best.model
summary(best_svm_model)
predictions <- predict(best_svm_model, newdata = test_data)
library(forestat)
library(e1071)
data("crassifolia")
# 数据处理
# AGB = Stem + Branch + Foliage + Fruit
crassifolia$AGB = crassifolia$Stem + crassifolia$Branch + crassifolia$Foliage + crassifolia$Fruit
set.seed(123)
datapartde <- sample(nrow(crassifolia), 0.7 * nrow(crassifolia))
train_data <- crassifolia[datapartde, ]
test_data <- crassifolia[-datapartde, ]
train_x <- train_data[, c("LH", "LHCB", "CPA", "D0", "H0", "HCB0", "CW")]
train_y <- train_data$AGB
test_x <- test_data[, c("LH", "LHCB", "CPA", "D0", "H0", "HCB0", "CW")]
test_y <- test_data$AGB
svm_tune <- tune.svm(AGB ~ LH + LHCB + CPA + D0 + H0 + HCB0 + CW,
data = train_data, type = "eps-regression",
kernel = "radial", cost = seq(1, 100, by = 1),
gamma = seq(0.001, 0.01, by = 0.001),
epsilon = seq(0.01, 0.1, by = 0.01))
best_svm_model <- svm_tune$best.model
summary(best_svm_model)
predictions <- predict(best_svm_model, newdata = test_data)
best_svm_model <- svm_tune$best.model
summary(best_svm_model)
# 性能评估
FittingEvaluationIndex(predictions, test_y)
svm.res <- predictions - test_y
data.svm <- data.frame(x = predictions, y = svm.res)
p.svm <- ggplot(data.svm, aes(x = x, y = y)) +
theme_light() +
geom_point(color = "steelblue", size = 3, show.legend = F) +
geom_hline(yintercept = c(0)) +
geom_vline(xintercept = c(0)) +
scale_x_continuous(limits = c(0, 400)) +
scale_y_continuous(limits = c(-200, 200)) +
labs(x = "Ground Estimated AGB (g/m2)", y = "Residual")
p.svm
View(data.svm)
p.svm <- ggplot(data.svm, aes(x = x, y = y)) +
theme_light() +
geom_point(color = "steelblue", size = 3, show.legend = F) +
geom_hline(yintercept = c(0)) +
geom_vline(xintercept = c(0)) +
scale_x_continuous(limits = c(0, 600)) +
scale_y_continuous(limits = c(-10, 10)) +
labs(x = "Ground Estimated AGB (g/m2)", y = "Residual") +
theme(text = element_text(size = 24))
p.svm
crassifolia$AGB = crassifolia$Stem + crassifolia$Branch + crassifolia$Foliage + crassifolia$Fruit
data <- crassifolia[,c("LH", "LHCB", "CPA", "D0", "H0", "HCB0", "CW", "AGB")]
data <- as.data.frame(cbind(scale(data[,-ncol(data)]), AGB = data$AGB))
dim(data)
set.seed(123)
datapartde <- sample(nrow(data), 0.7 * nrow(data))
train_data <- data[datapartde, ]
test_data <- data[-datapartde, ]
train_x <- train_data[, c("LH", "LHCB", "CPA", "D0", "H0", "HCB0", "CW")]
train_y <- train_data$AGB
test_x <- test_data[, c("LH", "LHCB", "CPA", "D0", "H0", "HCB0", "CW")]
test_y <- test_data$AGB
nn_model <- neuralnet(AGB ~ LH+LHCB+CPA+D0+H0+HCB0+CW,         #  LH+LHCB+CPA+D0+H0+HCB0+CW
data = train_data,     # 训练数据
hidden = c(10, 10),  # 设置为3个隐藏层
linear.output = TRUE,  # 设置为TRUE（回归模型）
err.fct = "sse",       # 使用平方误差损失函数
act.fct = "logistic",  # 激活函数使用logistic（也可以选择tanh等）
threshold = 0.1,       # 训练的停止标准，默认值通常足够
learningrate = 0.001,   # 学习率为0.01
stepmax = 400000
)
library(neuralnet)    # 神经网络
crassifolia$AGB = crassifolia$Stem + crassifolia$Branch + crassifolia$Foliage + crassifolia$Fruit
data <- crassifolia[,c("LH", "LHCB", "CPA", "D0", "H0", "HCB0", "CW", "AGB")]
data <- as.data.frame(cbind(scale(data[,-ncol(data)]), AGB = data$AGB))
dim(data)
set.seed(123)
datapartde <- sample(nrow(data), 0.7 * nrow(data))
train_data <- data[datapartde, ]
test_data <- data[-datapartde, ]
train_x <- train_data[, c("LH", "LHCB", "CPA", "D0", "H0", "HCB0", "CW")]
train_y <- train_data$AGB
test_x <- test_data[, c("LH", "LHCB", "CPA", "D0", "H0", "HCB0", "CW")]
test_y <- test_data$AGB
nn_model <- neuralnet(AGB ~ LH+LHCB+CPA+D0+H0+HCB0+CW,         #  LH+LHCB+CPA+D0+H0+HCB0+CW
data = train_data,     # 训练数据
hidden = c(10, 10),  # 设置为3个隐藏层
linear.output = TRUE,  # 设置为TRUE（回归模型）
err.fct = "sse",       # 使用平方误差损失函数
act.fct = "logistic",  # 激活函数使用logistic（也可以选择tanh等）
threshold = 0.1,       # 训练的停止标准，默认值通常足够
learningrate = 0.001,   # 学习率为0.01
stepmax = 400000
)
predictions <- predict(nn_model, newdata = test_data)
mse <- mean((predictions - test_y)^2)
rsq <- cor(predictions, test_y)^2
cat("测试集 MSE：", mse, "\n")
cat("测试集 R²：", rsq, "\n")
plot(model, rep = "best", col.entry = "blue", col.hidden = "green",
col.out = "red", arrow.length = 0.2, cex = 0.8, show.weights = FALSE)
plot(nn_model,
col.entry = "blue",       # 输入节点颜色
col.hidden = "green",     # 隐藏层节点颜色
col.out = "red",          # 输出节点颜色
arrow.length = 0.2,       # 调整箭头长度
cex = 0.8,                # 调整节点文本大小
information = FALSE,
show.weights = FALSE)
pdf("exp13-5神经网络.pdf", width = 10, height = 6)
plot(nn_model,
col.entry = "blue",       # 输入节点颜色
col.hidden = "green",     # 隐藏层节点颜色
col.out = "red",          # 输出节点颜色
arrow.length = 0.2,       # 调整箭头长度
cex = 0.8,                # 调整节点文本大小
information = FALSE,
show.weights = FALSE)
dev.off()
pdf("exp13-5神经网络.pdf", width = 10, height = 6)
plot(nn_model,
col.entry = "blue",       # 输入节点颜色
col.hidden = "green",     # 隐藏层节点颜色
col.out = "red",          # 输出节点颜色
arrow.length = 0.2,       # 调整箭头长度
cex = 0.8,                # 调整节点文本大小
information = FALSE,
show.weights = FALSE)
dev.off()
plot(nn_model,
col.entry = "blue",       # 输入节点颜色
col.hidden = "green",     # 隐藏层节点颜色
col.out = "red",          # 输出节点颜色
arrow.length = 0.2,       # 调整箭头长度
cex = 0.8,                # 调整节点文本大小
information = FALSE,
show.weights = FALSE)
summary(best_svm_model)
pdf("exp13-5神经网络.pdf", width = 10, height = 6)
plot(nn_model,
col.entry = "blue",       # 输入节点颜色
col.hidden = "green",     # 隐藏层节点颜色
col.out = "red",          # 输出节点颜色
arrow.length = 0.2,       # 调整箭头长度
cex = 0.8,                # 调整节点文本大小
information = FALSE,
show.weights = FALSE)
dev.off()
predictions <- predict(nn_model, newdata = test_data)
mse <- mean((predictions - test_y)^2)
rsq <- cor(predictions, test_y)^2
cat("测试集 MSE：", mse, "\n")
cat("测试集 R²：", rsq, "\n")
plot(model)
model <- neuralnet(Species ~ . , data = iris, hidden = c(5, 3), linear.output = FALSE)
plot(model)
plot(model)
?mlp
install.packages("【重要】2025年全国硕士研究生招生考试大连理工大学（盘锦校区）考点公告https://pjteach.dlut.edu.cn/info/1045/7978.htm
各位班长，教务发布了研究生入学考试 入场及离场要求，请转发给全体同学。辛苦同学们反复研读《考点公共》，建议大家带好必备用品，轻装上阵；今年候场和安检变化大，一定提前熟悉好考场位置，避免耽误入场。另外，同寝室同学，最好结伴出行，大家可互相提醒下，千万别睡过头。")
install.packages("mlp")
source("load_preprocess.R")
setwd("D:\\大连理工大学\\R语言书稿\\Rforestat\\R_code\\chap13")
source("load_preprocess.R")
#  神经网络
library(neuralnet)
# 1. 数据预处理
library(RSNNS)
maxs <- apply(trainde_m1, 2, max)
mins <- apply(trainde_m1, 2, min)
scaled <- as.data.frame(scale(trainde_m1, center = mins,
scale = maxs - mins))
index <- sample(1:nrow(trainde_m1), round(0.75 * nrow(trainde_m1)))
train_<- scaled[datapartde==1,]
test_<- scaled[datapartde==2,]
nn <- neuralnet(agb~.,
data = train_, hidden = c(20),
linear.output = TRUE)
View(scaled)
neuralnet::compute(nn,train_[,2:101])
View(train_)
View(train_)
?createFolds
library(foreach)
library(doParallel)
library(neuralnet)
library(forestat)
library(caret)
data("crassifolia")
# AGB = Stem + Branch + Foliage + Fruit
crassifolia$AGB = crassifolia$Stem + crassifolia$Branch + crassifolia$Foliage + crassifolia$Fruit
data <- crassifolia[,c("LH", "LHCB", "CPA", "D0", "H0", "HCB0", "CW", "AGB")]
data <- as.data.frame(cbind(scale(data[,-ncol(data)]), AGB = data$AGB))
set.seed(123)
datapartde <- sample(nrow(data), 0.7 * nrow(data))
train_data <- data[datapartde, ]
test_data <- data[-datapartde, ]
train_x <- train_data[, c("LH", "LHCB", "CPA", "D0", "H0", "HCB0", "CW")]
train_y <- train_data$AGB
test_x <- test_data[, c("LH", "LHCB", "CPA", "D0", "H0", "HCB0", "CW")]
test_y <- test_data$AGB
cv_folds <- createFolds(train_data$AGB, k = folds, list = TRUE, returnTrain = TRUE)
cv_folds <- createFolds(train_data$AGB, k = 5, list = TRUE, returnTrain = TRUE)
View(cv_folds)
cv_folds[["Fold1"]]
cv_folds <- createFolds(train_data$AGB, k = 5, list = F, returnTrain = TRUE)
cv_folds <- createFolds(train_data$AGB, k = 5, list = T, returnTrain = TRUE)
View(cv_folds)
cv_folds <- createFolds(train_data$AGB, k = 5, list = F, returnTrain = TRUE)
?lapply
sapply(cv_folds, length)
cv_folds <- createFolds(train_data$AGB, k = 5, list = T, returnTrain = TRUE)
sapply(cv_folds, length)
train_data.length()
nrow(train_data)
my_list <- list(a = 1:5, b = 6:10, c = 11:15)
result <- lapply(my_list, funtion(x) print(x))
my_list <- list(a = 1:5, b = 6:10, c = 11:15)
# 使用 lapply
result <- lapply(my_list, function(x) {
print(paste("当前元素:", x))
sum(x)  # 对当前元素求和
})
my_list <- list(a = 1:5, b = 6:10, c = 11:15)
# 使用 lapply
result <- lapply(my_list, function(x) {
print(x)
sum(x)  # 对当前元素求和
})
View(result)
my_list[1]
my_list[[1]
]
my_list[[1]]
tune_grid <- expand.grid(
hidden = list(c(10, 10)),  # 隐藏层结构
learningrate = seq(0.0001, 0.001, by = 0.0001),         # 学习率
threshold = c(0.01, 0.001),            # 停止条件
stepmax = c(1e5, 1e6)                  # 最大步数
)
?mlp
?plotIterativeError
?plotRegressionError
library(foreach)
library(doParallel)
library(neuralnet)
library(forestat)
library(caret)
data("crassifolia")
# AGB = Stem + Branch + Foliage + Fruit
crassifolia$AGB = crassifolia$Stem + crassifolia$Branch + crassifolia$Foliage + crassifolia$Fruit
data <- crassifolia[,c("LH", "LHCB", "CPA", "D0", "H0", "HCB0", "CW", "AGB")]
data <- as.data.frame(cbind(scale(data[,-ncol(data)]), AGB = data$AGB))
set.seed(123)
datapartde <- sample(nrow(data), 0.7 * nrow(data))
train_data <- data[datapartde, ]
test_data <- data[-datapartde, ]
train_x <- train_data[, c("LH", "LHCB", "CPA", "D0", "H0", "HCB0", "CW")]
train_y <- train_data$AGB
test_x <- test_data[, c("LH", "LHCB", "CPA", "D0", "H0", "HCB0", "CW")]
test_y <- test_data$AGB
# 设置并行计算的核心数
num_cores <- 4  # detectCores()
cl <- makeCluster(num_cores)
registerDoParallel(cl)
# 定义五折交叉验证函数
cross_validate_neuralnet <- function(data, tune_grid, folds = 5) {
# 创建 K 折
cv_folds <- createFolds(data$AGB, k = folds, list = TRUE, returnTrain = TRUE)
results <- foreach(params = iter(tune_grid, by = "row"), .combine = rbind, .packages = c("neuralnet", "caret")) %dopar% {
fold_results <- lapply(cv_folds, function(train_index) {
# 划分训练集和验证集
train_data <- data[train_index, ]
test_data <- data[-train_index, ]
# 训练神经网络模型
nn_model <- tryCatch({
neuralnet(
AGB ~ LH + LHCB + CPA + D0 + H0 + HCB0 + CW,
data = train_data,
hidden = unlist(params["hidden"]),
linear.output = TRUE,
err.fct = "sse",
act.fct = "logistic",
threshold = as.numeric(params["threshold"]),
learningrate = as.numeric(params["learningrate"]),
stepmax = as.numeric(params["stepmax"])
)
}, error = function(e) {
return(NULL)
})
# 若模型训练失败，返回空结果
if (is.null(nn_model)) return(data.frame(rmse = NA, r_squared = NA))
# 在验证集上预测并计算性能指标
predictions <- compute(nn_model, test_data[, c("LH", "LHCB", "CPA", "D0", "H0", "HCB0", "CW")])$net.result
rmse <- sqrt(mean((predictions - test_data$AGB)^2))
r_squared <- cor(predictions, test_data$AGB)^2
return(data.frame(rmse = rmse, r_squared = r_squared))
})
# 汇总所有折的性能
fold_results <- do.call(rbind, fold_results)
avg_rmse <- mean(fold_results$rmse, na.rm = TRUE)
avg_r_squared <- mean(fold_results$r_squared, na.rm = TRUE)
# 返回当前参数组合的平均性能
data.frame(
hidden = paste(unlist(params["hidden"]), collapse = ","),
learningrate = as.numeric(params["learningrate"]),
threshold = as.numeric(params["threshold"]),
stepmax = as.numeric(params["stepmax"]),
avg_rmse = avg_rmse,
avg_r_squared = avg_r_squared
)
}
return(results)
}
# 设置超参数网格
tune_grid <- expand.grid(
hidden = list(c(10, 10)),  # 隐藏层结构
learningrate = seq(0.0001, 0.001, by = 0.0001),         # 学习率
threshold = c(0.01, 0.001),            # 停止条件
stepmax = c(1e5, 1e6)                  # 最大步数
)
# 执行网格搜索
grid_search_results <- cross_validate_neuralnet(data = train_data, tune_grid = tune_grid, folds = 5)
