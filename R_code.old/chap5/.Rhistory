KNNModel_H <- caret::train(x = scale(trainde_m1[, 2:101]), y = trainde_m1[, 1],
data = trainde_m1,
method = "knn",
tuneGrid = grid,
returnData = TRUE,
returnResamp = "all",
trControl = fitControl,
savePredictions = TRUE)
KNNModel_H
# 模型评估
knn_pred1 <- predict(KNNModel_H)
FittingEvaluationIndex(knn_pred1, trainde_m1$agb)
# 在测试集上进行预测和评估
knn_pred2 <- predict(KNNModel_H, newdata = testde_m1)
FittingEvaluationIndex(knn_pred2, testde_m1$agb)
# 绘制残差图
knn.res <- knn_pred2 - testde_m1$agb
data.knn <- data.frame(x = knn_pred2, y = knn.res)
p.knn <- ggplot(testde_m1, aes(x = knn_pred2, y = knn.res)) +
theme_light() +
geom_point(color = "steelblue", size = 3
, show.legend = F) +
geom_hline(yintercept = c(0)) +
geom_vline(xintercept = c(0)) +
scale_x_continuous(limits = c(0, 400)) +
scale_y_continuous(limits = c(-200, 200)) +
labs(x = "Ground extimated AGB(g/m2)", y = "Residual")
p.knn
# 加载必要的库
library(mgcv)
library(ggplot2)
library(Metrics)
library(car)
library(broom)
library(cowplot)
library(pdp)
library(gridExtra)
# 定义模型评估函数
FittingEvaluationIndex <- function(EstiH, ObsH) {
Index <- array(dim=5)
e <- ObsH - EstiH
e1 <- ObsH - mean(ObsH)
pe <- mean(e)
var2 <- var(e)
RMSE <- sqrt(pe^2 + var2)
R2 <- 1 - sum(e^2) / sum((e1)^2)
TRE <- 100 * sum(e^2) / sum((EstiH)^2)
Index[1] <- pe
Index[2] <- RMSE
Index[3] <- R2
Index[4] <- var2
Index[5] <- TRE
dimnames(Index) <- list(c("pe", "RMSE", "R2", "Var", "TRE"))
return(Index)
}
# 读取数据
sm21 <- read.csv('la_m.csv', header=TRUE)
library(caret)
# 加载数据和预处理函数
source("load_preprocess.R")
setwd("D:\\大连理工大学\\R语言书稿\\Rforestat\\R_code\\chap13")
# 加载数据和预处理函数
source("load_preprocess.R")
# KNN模型调参
grid <- expand.grid(k = seq(1, 20, by = 1))
fitControl <- trainControl(method = "LOOCV", number = 10)
# 训练KNN模型
KNNModel_H <- caret::train(x = scale(trainde_m1[, 2:101]), y = trainde_m1[, 1],
method = "knn",
tuneGrid = grid,
returnData = TRUE,
returnResamp = "all",
trControl = fitControl,
savePredictions = TRUE)
library(caret)
# 加载数据和预处理函数
source("load_preprocess.R")
# KNN模型调参
grid <- expand.grid(k = seq(1, 20, by = 1))
fitControl <- trainControl(method = "LOOCV", number = 10)
# 训练KNN模型
KNNModel_H <- caret::train(x = scale(trainde_m1[, 2:101]), y = trainde_m1[, 1],
method = "knn",
tuneGrid = grid,
returnData = TRUE,
returnResamp = "all",
trControl = fitControl,
savePredictions = TRUE)
KNNModel_H
# 模型评估
knn_pred1 <- predict(KNNModel_H)
FittingEvaluationIndex(knn_pred1, trainde_m1$agb)
# 加载数据和预处理函数
source("load_preprocess.R")
grid <- expand.grid(k = seq(1, 20, by = 1))
grid <- expand.grid(k = seq(1, 20, by = 1))
fitControl <- trainControl(method = "LOOCV", number = 10)
# 训练KNN模型
KNNModel_H <- caret::train(x = scale(trainde_m1[, 2:101]), y = trainde_m1[, 1],
data = trainde_m1,
method = "knn",
tuneGrid = grid,
returnData = TRUE,
returnResamp = "all",
trControl = fitControl,
savePredictions = TRUE)
KNNModel_H
# 模型评估
knn_pred1 <- predict(KNNModel_H)
FittingEvaluationIndex(knn_pred1, trainde_m1$agb)
# 在测试集上进行预测和评估
knn_pred2 <- predict(KNNModel_H, newdata = testde_m1)
FittingEvaluationIndex(knn_pred2, testde_m1$agb)
# 绘制残差图
knn.res <- knn_pred2 - testde_m1$agb
data.knn <- data.frame(x = knn_pred2, y = knn.res)
p.knn <- ggplot(testde_m1, aes(x = knn_pred2, y = knn.res)) +
theme_light() +
geom_point(color = "steelblue", size = 3
, show.legend = F) +
geom_hline(yintercept = c(0)) +
geom_vline(xintercept = c(0)) +
scale_x_continuous(limits = c(0, 400)) +
scale_y_continuous(limits = c(-200, 200)) +
labs(x = "Ground extimated AGB(g/m2)", y = "Residual")
p.knn
library(caret)
# 加载数据和预处理函数
source("load_preprocess.R")
grid <- expand.grid(k = seq(1, 20, by = 1))
fitControl <- trainControl(method = "LOOCV", number = 10)
# 训练KNN模型
KNNModel_H <- caret::train(x = scale(trainde_m1[, 2:101]), y = trainde_m1[, 1],
data = trainde_m1,
method = "knn",
tuneGrid = grid,
returnData = TRUE,
returnResamp = "all",
trControl = fitControl,
savePredictions = TRUE)
KNNModel_H
# 模型评估
knn_pred1 <- predict(KNNModel_H)
FittingEvaluationIndex(knn_pred1, trainde_m1$agb)
# 在测试集上进行预测和评估
knn_pred2 <- predict(KNNModel_H, newdata = testde_m1)
FittingEvaluationIndex(knn_pred2, testde_m1$agb)
# 绘制残差图
knn.res <- knn_pred2 - testde_m1$agb
data.knn <- data.frame(x = knn_pred2, y = knn.res)
p.knn <- ggplot(testde_m1, aes(x = knn_pred2, y = knn.res)) +
theme_light() +
geom_point(color = "steelblue", size = 3
, show.legend = F) +
geom_hline(yintercept = c(0)) +
geom_vline(xintercept = c(0)) +
scale_x_continuous(limits = c(0, 400)) +
scale_y_continuous(limits = c(-200, 200)) +
labs(x = "Ground extimated AGB(g/m2)", y = "Residual")
p.knn
source("load_preprocess.R")
# 随机森林模型调参
library(randomForest)
value <- seq(1, 101, by = 1)
rfModel_bestmtry <- list()
for (i in value) {
set.seed(1)
train_rf <- randomForest(x = trainde_m[, 2:102], y = trainde_m[, 1],
ntree = 1000, mtry = i, importance = TRUE, proximity = TRUE)
rfModel_bestmtry[[i]] <- train_rf$rsq[[1000]]
i <- i + 1
}
best_mtry <- which.max(rfModel_bestmtry)
print(best_mtry)
rfModel_bestmtry
# 训练随机森林模型
set.seed(1)
train_rf <- randomForest(x = trainde_m[, 2:102], y = trainde_m[, 1],
ntree = 1000, mtry = 17, importance = TRUE, proximity = TRUE)
# 模型评估
FittingEvaluationIndex(predict(train_rf), trainde_m$agb)
# 在测试集上进行预测和评估
test_pred <- predict(train_rf, newdata = testde_m)
FittingEvaluationIndex(test_pred, testde_m$agb)
# 绘制残差图
library(ggplot2)
rf.res <- test_pred - testde_m$agb
data.rf <- data.frame(x = test_pred, y = rf.res)
p.rf <- ggplot(testde_m, aes(x = test_pred, y = rf.res)) +
theme_light() +
geom_point(color = "steelblue", size = 3, show.legend = F) +
geom_hline(yintercept = c(0)) +
geom_vline(xintercept = c(0)) +
scale_x_continuous(limits = c(0, 400)) +
scale_y_continuous(limits = c(-200, 200)) +
labs(x = "Ground extimated AGB(g/m2)", y = "Residual")
p.rf
source("load_preprocess.R")
library(gbm)
j <- seq(100, 5000, by = 100)
pred <- data.frame()
gbm.HCB<- gbm(agb~.,
distribution = "gaussian",
data = trainde_m1,
n.trees = 4000,
interaction.depth = 1,
shrinkage = 0.001,
bag.fraction = 0.5,
)
plot(gbm.HCB,xlab="迭代次数 Boosting Iterations",ylab="RMSE/m")
gbm_pred1 <- predict(gbm.HCB)
FittingEvaluationIndex(gbm_pred1,trainde_m1$agb)
gbm_pred2 <- predict(gbm.HCB,newdata = testde_m1)
FittingEvaluationIndex(gbm_pred2,testde_m1$agb)
# 4 残差分析
gbm.res <- gbm_pred2-testde_m1$agb;
# 4 残差分析
gbm.res <- gbm_pred2-testde_m1$agb
data.gbm<- data.frame(x=gbm_pred2,y=gbm.res)
p.gbm <- ggplot(data.gbm, aes(x=gbm_pred2,y=gbm.res,color="z",cex=2)) +
geom_point(,show.legend = F)+geom_hline(yintercept = c(0))+
labs(x="Ground extimated AGB(g/m2)",y="Residual")
p.gbm
library(dplyr)
library(earth)
hyper_grid <- expand.grid(
degree = 1:3,
nprune = seq(2, 100, length.out = 10) %>% floor()
)
cv_H <- caret::train(agb~.,data=trainde_m1,
method = "earth",
metric = "RMSE",
tuneGrid = hyper_grid
)
source("load_preprocess.R")
library(dplyr)
library(earth)
hyper_grid <- expand.grid(
degree = 1:3,
nprune = seq(2, 100, length.out = 10) %>% floor()
)
cv_H <- caret::train(agb~.,data=trainde_m1,
method = "earth",
metric = "RMSE",
tuneGrid = hyper_grid
)
cv_H
# 4. 调优结果可视化
ggplot(cv_H)+labs(x="nprune",y="RMSE/m")
# 5. 模型手动拟合
earth.Ha <- earth(agb~., data = trainde_m1,nprune=12,degree=2)
# 6.模型可视化与评估
plot(earth.Ha)
FittingEvaluationIndex(predict(earth.Ha),trainde_m1$agb)
FittingEvaluationIndex(predict(earth.Ha,newdata = testde_m1),testde_m1$agb)
source("load_preprocess.R")
# SVM模型调参
library(e1071)
trainde_m1 <- trainde_m[, -c(29)]
testde_m1 <- testde_m[, -c(29)]
# SVM模型调参
library(e1071)
set.seed(1)
obj <- tune.svm(x = trainde_m1[, 2:101], y = trainde_m1[, 1],
kernel = 'radial',
gamma = c(0.00001,0.0001,0.001,0.01,0.1,0.1,c(seq(from = 1, to = 5, by = 1))),
cost =c(seq(from = 1, to = 40, by = 1)))
summary(obj)
dev.new()
plot(obj)
dev.new()
plot(obj)
plot(obj)
# 训练SVM模型
set.seed(1)
svm_model <- svm(x = trainde_m1[, 2:101], y = trainde_m1[, 1],
trControl = controlObject,
importance = TRUE,
tkernel = 'radial', gamma = 0.001, cost = 13)
# 模型评估
svm_pred1 <- predict(svm_model)
FittingEvaluationIndex(svm_pred1, trainde_m1$agb)
# 在测试集上进行预测和评估
svm_pred2 <- predict(svm_model, newdata = testde_m1[, 2:101])
FittingEvaluationIndex(svm_pred2, testde_m1$agb)
# 绘制残差图
svm.res <- svm_pred2 - testde_m1$agb
data.svm <- data.frame(x = svm_pred2, y = svm.res)
p.svm <- ggplot(testde_m1, aes(x = svm_pred2, y = svm.res)) +
theme_light() +
geom_point(color = "steelblue", size = 3, show.legend = F) +
geom_hline(yintercept = c(0)) +
geom_vline(xintercept = c(0)) +
scale_x_continuous(limits = c(0, 400)) +
scale_y_continuous(limits = c(-200, 200)) +
labs(x = "Ground extimated AGB(g/m2)", y = "Residual")
p.svm
source("load_preprocess.R")
# SVM模型调参
library(e1071)
set.seed(1)
obj <- tune.svm(x = trainde_m1[, 2:101], y = trainde_m1[, 1],
kernel = 'radial',
gamma = c(0.00001,0.0001,0.001,0.01,0.1,0.1,c(seq(from = 1, to = 5, by = 1))),
cost =c(seq(from = 1, to = 40, by = 1)))
summary(obj)
# 训练SVM模型
set.seed(1)
svm_model <- svm(x = trainde_m1[, 2:101], y = trainde_m1[, 1],
trControl = controlObject,
importance = TRUE,
tkernel = 'radial', gamma = 0.001, cost = 13)
View(svm_model)
# 训练SVM模型
controlObject <- trainControl(method = "repeatedcv",
repeats = 2,
number = 10)
set.seed(1)
svm_model <- svm(x = trainde_m1[, 2:101], y = trainde_m1[, 1],
trControl = controlObject,
importance = TRUE,
tkernel = 'radial', gamma = 0.001, cost = 13)
# 模型评估
svm_pred1 <- predict(svm_model)
FittingEvaluationIndex(svm_pred1, trainde_m1$agb)
# 在测试集上进行预测和评估
svm_pred2 <- predict(svm_model, newdata = testde_m1[, 2:101])
FittingEvaluationIndex(svm_pred2, testde_m1$agb)
# 绘制残差图
svm.res <- svm_pred2 - testde_m1$agb
data.svm <- data.frame(x = svm_pred2, y = svm.res)
p.svm <- ggplot(testde_m1, aes(x = svm_pred2, y = svm.res)) +
theme_light() +
geom_point(color = "steelblue", size = 3, show.legend = F) +
geom_hline(yintercept = c(0)) +
geom_vline(xintercept = c(0)) +
scale_x_continuous(limits = c(0, 400)) +
scale_y_continuous(limits = c(-200, 200)) +
labs(x = "Ground extimated AGB(g/m2)", y = "Residual")
p.svm
source("load_preprocess.R")
library(neuralnet)
maxs <- apply(trainde_m1, 2, max)
mins <- apply(trainde_m1, 2, min)
scaled <- as.data.frame(scale(trainde_m1, center = mins,
scale = maxs - mins))
nn <- neuralnet(agb~.,
data = trainde_m1, hidden = c(20),
linear.output=TRUE)
train_result <- neuralnet::compute(nn,trainde_m1[,2:101])$net.result
FittingEvaluationIndex(train_result,trainde_m1[,1])
test_result <- neuralnet::compute(nn, testde_m1[,2:101])$net.result
FittingEvaluationIndex(test_result,testde_m1[,1])
fitControl=trainControl(method="repeatedcv",number=10)
tunedf=expand.grid(.decay=c(0.01:0.1),.size=c(4:15),.bag=TRUE)
nnetmodel.H <- caret::train(agb~.,
data=trainde_m1,method='avNNet',
preProc = "range",
trace=FALSE,linout=FALSE,tuneGrid=tunedf)
nnet_pred1 <- predict(nnetmodel.H)
FittingEvaluationIndex(nnet_pred1,trainde_m1$agb)
nnet_pred2 <- predict(nnetmodel.H,newdata = testde_m1)
FittingEvaluationIndex(nnet_pred2,testde_m1$agb)
library(RSNNS)
BPNNreg <- mlp(trainde_m1[,2:101], trainde_m1[,1], maxit=100,
inputsTest=testde_m1[,2:101],
targetsTest=testde_m1[,1],
metric="RMSE")
BPNNreg
plotRegressionError(trainde_m1$agb, BPNNreg$fitted.values, main="BPNN regssion train fit")
FittingEvaluationIndex(trainde_m1$agb, BPNNreg$fitted.values)
########--------------5、神经网络 --------------#########
library(RSNNS)
maxs <- apply(trainde_m1, 2, max)
mins <- apply(trainde_m1, 2, min)
scaled <- as.data.frame(scale(trainde_m1, center = mins,
scale = maxs - mins))
index <- sample(1:nrow(trainde_m1), round(0.75 * nrow(trainde_m1)))
train_<- scaled[datapartde==1,]
test_<- scaled[datapartde==2,]
nn <- neuralnet(agb~.,
data = train_, hidden = c(20),
linear.output = TRUE)
train_result <- neuralnet::compute(nn,train_[,2:101])$net.result
FittingEvaluationIndex(train_result,train_[,1])
test_result <- neuralnet::compute(nn, test_[,2:101])$net.result
FittingEvaluationIndex(test_result,test_[,1])
fitControl=trainControl(method="repeatedcv",number=10)
tunedf=expand.grid(.decay=c(0.01:0.1),.size=c(4:15),.bag=TRUE)
####something wrong here
caret::train(agb~.,data=train_,method='avNNet',
trControl=fitControl,trace=FALSE,
linout=FALSE,tuneGrid=tunedf)
nnetmodel.H <- caret::train(agb~.,
data=trainde_m1,method='avNNet',
preProc = "range",
trace=FALSE,linout=FALSE,tuneGrid=tunedf)
nnetmodel.H
BPNNreg <- mlp(trainde_m1[,2:101], trainde_m1[,1], maxit=100,
inputsTest=testde_m1[,2:101],
targetsTest=testde_m1[,1],
metric="RMSE")
BPNNreg
summary(BPNNreg)
#par(cex=0.6)
#plotnet(BPNNreg, pos_col="red", neg_col="grey")
plotIterativeError(BPNNreg, main="BPNN回归迭代误差")
plotRegressionError(trainde_m1$agb, BPNNreg$fitted.values, main="BPNN regssion train fit")
FittingEvaluationIndex(trainde_m1$agb, BPNNreg$fitted.values)
####something wrong here
train(agb~.,data=train_,method='avNNet',
trControl=fitControl,trace=FALSE,
linout=FALSE,tuneGrid=tunedf)
####something wrong here
caret::train(agb~.,data=train_,method='avNNet',
trControl=fitControl,trace=FALSE,
linout=FALSE,tuneGrid=tunedf)
nnetmodel.H <- caret::train(agb~.,
data=trainde_m1,method='avNNet',
preProc = "range",
trace=FALSE,linout=FALSE,tuneGrid=tunedf)
source("load_preprocess.R")
#  神经网络
# 1. 数据预处理
library(RSNNS)
maxs <- apply(trainde_m1, 2, max)
mins <- apply(trainde_m1, 2, min)
scaled <- as.data.frame(scale(trainde_m1, center = mins,
scale = maxs - mins))
index <- sample(1:nrow(trainde_m1), round(0.75 * nrow(trainde_m1)))
train_<- scaled[datapartde==1,]
test_<- scaled[datapartde==2,]
nn <- neuralnet(agb~.,
data = train_, hidden = c(20),
linear.output = TRUE)
# 2. 神经网络模型的建立与训练
train_result <- neuralnet::compute(nn,train_[,2:101])$net.result
FittingEvaluationIndex(train_result,train_[,1])
test_result <- neuralnet::compute(nn, test_[,2:101])$net.result
FittingEvaluationIndex(test_result,test_[,1])
# 3. 参数优化
fitControl=trainControl(method="repeatedcv",number=10)
tunedf=expand.grid(.decay=c(0.01:0.1),.size=c(4:15),.bag=TRUE)
####something wrong here
# caret::train(agb~.,data=train_,method='avNNet',
#              trControl=fitControl,trace=FALSE,
#              linout=FALSE,tuneGrid=tunedf)
nnetmodel.H <- caret::train(agb~.,
data=trainde_m1,method='avNNet',
preProc = "range",
trace=FALSE,linout=FALSE,tuneGrid=tunedf)
nnetmodel.H
# 4. 模型性能评估
BPNNreg <- mlp(trainde_m1[,2:101], trainde_m1[,1], maxit=100,
inputsTest=testde_m1[,2:101],
targetsTest=testde_m1[,1],
metric="RMSE")
BPNNreg
summary(BPNNreg)
FittingEvaluationIndex(trainde_m1$agb, BPNNreg$fitted.values)
# 5. 模型可视化
#par(cex=0.6)
#plotnet(BPNNreg, pos_col="red", neg_col="grey")
plotIterativeError(BPNNreg, main="BPNN回归迭代误差")
plotRegressionError(trainde_m1$agb, BPNNreg$fitted.values, main="BPNN regssion train fit")
library(VIM)
radar_data1 <- kNN(radar_data1, imp_var=FALSE)
library(forestat)
View(trainde_m)
devtools::install_local("D:\\大连理工大学\\R语言书稿\\R包\\forestat-main\\forestat_1.1.0.tar.gz")
detach("package:forestat", unload = TRUE)
devtools::install_local("D:\\大连理工大学\\R语言书稿\\R包\\forestat-main\\forestat_1.1.0.tar.gz")
data(package="forestat")
help(forestat)
??forestat
data(crassifolia)
data(crassifolia)
data(birch)
library(forestat)
data(crassifolia)
crassifolia
crassifolia[1:5,]
setwd("D:\\大连理工大学\\R语言书稿\\R包\\forestat-main\\forestat")
# 确保DESCRIPTION中LazyData: TRUE，允许在加载包时自动延迟加载数据（即可用data()加载数据）
devtools::document()
devtools::build()
detach("package:forestat", unload = TRUE)
devtools::install_local("D:\\大连理工大学\\R语言书稿\\R包\\forestat-main\\forestat_1.1.0.tar.gz", force = T)
library(forestat)
#data(package = "forestat") # 查看包中包含的数据集
data(larch)
library(forestat)
#雷达反演数据.xlsx即为data3-1.xlsx
data(crassifolia)
radar_data <- crassifolia
dim(radar_data)
library(forestat)
#雷达反演数据.xlsx即为data3-1.xlsx
data(crassifolia)
radar_data <- crassifolia
dim(radar_data)
View(crassifolia)
data(birch)
force(birch)
View(birch)
data1=read.csv("data-{30m}-5-3.csv",sheet = 1, colNames = T)
data1=read.csv("data-{30m}-5-3.csv",header =TRUE, sep = "\t")
setwd("D:\\大连理工大学\\R语言书稿\\Rforestat\\R_code\\chap5")
data1=read.csv("data-{30m}-5-3.csv",header =TRUE, sep = "\t")
View(data1)
data1=read.csv("data-{30m}-5-3.csv",header =TRUE, sep = "\t",fileEncoding = "UTF-8-BOM")
View(data1)
